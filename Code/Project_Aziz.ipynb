{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T01:22:54.586170Z",
     "start_time": "2024-12-03T01:22:40.715684Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWELFake_Dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('WELFake_Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dr/l2qpm6ss6q58tg4prmqtdn7m0000gn/T/ipykernel_6959/1241197973.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot Distribution of Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot Distribution of Data\n",
    "\n",
    "plt.bar(label.index, label.values, color=['red','blue'])\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Disturbution of Fake vs Real News')\n",
    "plt.xticks([0, 1], [\"0=Real\", \"1=Fake\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove first column\n",
    "\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of missing values from dataframe\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null values with space\n",
    "\n",
    "df.fillna(' ', inplace=True)\n",
    "df.isnull().sum()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Text\n",
    " \n",
    "import re\n",
    "import nltk     \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    clean_text = str(text)\n",
    "    \n",
    "    clean_text = re.sub(r'[^a-zA-Z\\s]','',clean_text) # Remove punctuation, numbers, special characters\n",
    "    \n",
    "    clean_text = clean_text.lower()  # Convert text to lowercase\n",
    "\n",
    "    tokens = clean_text.split() # Perform tokenization on text\n",
    "    \n",
    "    stemm_tokens = [stemmer.stem(word) for word in tokens] # perform stemming on the tokens\n",
    "    \n",
    "    stop_words = set(stopwords.words('english')) # Get stopwords\n",
    "    clean_text = [word for word in stemm_tokens if word not in stop_words] # remove stopwords from list of words\n",
    "    \n",
    "    \n",
    "    return \" \".join(clean_text) # Join list of words back into a string\n",
    "\n",
    "df['title'] = df['title'].apply(clean) # Apply clean function to title column\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Text Column\n",
    "\n",
    "df.drop(columns=['text'], inplace=True, axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer Object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and Transform Data\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Initalize TruncatedSVD\n",
    "# Retain 1000 Features\n",
    "\n",
    "\n",
    "svd = TruncatedSVD(n_components=1000)\n",
    "\n",
    "# Apply on Train data\n",
    "X_train = svd.fit_transform(X_train)\n",
    "\n",
    "# Apply on Test data\n",
    "X_test = svd.transform(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Input to Hidden Layer\n",
    "        self.input = nn.Linear(1000, 500)  # 1000 Inputs, 500 out\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.norm1 = nn.BatchNorm1d(500)  # Batch normalization\n",
    "        self.drop1 = nn.Dropout(0.2)  # Dropout\n",
    "\n",
    "        # Hidden Layer 1 to Hidden Layer 2\n",
    "        self.hidden1 = nn.Linear(500, 250)  # 500 Inputs, 250 out\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.norm2 = nn.BatchNorm1d(250)  # Batch normalization\n",
    "        self.drop2 = nn.Dropout(0.2)  # Dropout\n",
    "\n",
    "        # Hidden Layer 2 to Hidden Layer 3\n",
    "        self.hidden2 = nn.Linear(250, 50)  # 250 Inputs, 50 out\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.norm3 = nn.BatchNorm1d(50)  # Batch normalization\n",
    "        self.drop3 = nn.Dropout(0.2)  # Dropout\n",
    "\n",
    "        # Hidden Layer 3 to Output\n",
    "        self.hidden3 = nn.Linear(50, 1)  # 50 Inputs, 1 out\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input Layer\n",
    "        x = self.input(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        # Hidden Layer 1\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        # Hidden Layer 2\n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.drop3(x)\n",
    "\n",
    "        # Hidden Layer 3 (Output)\n",
    "        x = self.hidden3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize Model\n",
    "model = NeuralNetwork()\n",
    "\n",
    "# Initialize ADAM optimizer, to update weights of the model\n",
    "optimize = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Define Loss Function (Binary Cross Entropy)\n",
    "error = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Dataset to pytorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dr/l2qpm6ss6q58tg4prmqtdn7m0000gn/T/ipykernel_6959/590453262.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Forward Propagation (calling the forward method of the neural network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Convert to 1D to match Y_train shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "# Loop for number of Epochs\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    \n",
    "    # Forward Propagation (calling the forward method of the neural network)\n",
    "    outputs = model(X_train)\n",
    "    # Convert to 1D to match Y_train shape\n",
    "    outputs = outputs.squeeze(1)\n",
    "    \n",
    "    \n",
    "    # Compute Loss between outputs and true labels\n",
    "    loss = error(outputs, y_train)  # error is the loss function defined earlier\n",
    "    \n",
    "    \n",
    "    # Clears gradients before computing the new gradients for the next iteration \n",
    "    optimize.zero_grad()\n",
    "    \n",
    "    # Backward propogation \n",
    "    # Computes gradients of the loss function with respect to the parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Uses the chosen optimizer (Adam) to adjust the model's weights based on the gradients calculated in the backpropagation step\n",
    "    optimize.step()\n",
    "    \n",
    "    print(f\"Loss after iteration {epoch} = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():     # Disable Gradient Computation\n",
    "    # Perform forward pass using X_test data\n",
    "    # Shape is [batch_size, 1]\n",
    "    y_pred = model(X_test)  \n",
    "    print(y_pred[0])\n",
    "    \n",
    "    # Convert y_pred output into labels (0 or 1)\n",
    "    # reshape from [batch_size, 1] to [batch_size]\n",
    "    # if greater than 0 (True) then class 1, less than or equal to 0 (False) then class 0\n",
    "    # Turn boolean into integer values\n",
    "    y_pred_labels = (y_pred.squeeze(1) > 0).int()\n",
    "    print(y_pred_labels[0])\n",
    "    \n",
    "    # Calculate Accuracy \n",
    "    # compare predicted labels with actual labels \n",
    "    # divides by the total number of samples in y_test \n",
    "    accuracy = (y_pred_labels == y_test).sum().item() / y_test.size(0)\n",
    "    print(f\"Accuracy of the model is {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute confusion matrix. Comparing true with predicyed labels\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred_labels)\n",
    "\n",
    "# Display confusion matrix\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
